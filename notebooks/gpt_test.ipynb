{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sksms\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional_encoding.py\n",
    "from src.tokenizer import BytePairTokenizer, load_tokenizer\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [1777, 4313, 2964, 4313, 3279, 2804, 3914, 3066, 4889, 2871, 10120, 2896, 3070, 3399, 3182, 3474, 5091, 2765, 2963, 3001, 3580, 2796, 3181, 10557, 3698, 3496, 2854, 3874, 4855, 2837, 2871, 7153, 5263, 2772, 5468, 2893, 7311, 3175, 2764, 3175, 2764, 10580]\n",
      "Decoded: Sean Bean has a hard time leaving his role as Eddard Stark . He vows to get revenge against those that assisted in his execution , starting with George R. R. Martin\n"
     ]
    }
   ],
   "source": [
    "# 1. 필요한 모듈 임포트\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import math\n",
    "\n",
    "# Tokenizer 테스트\n",
    "tokenizer = load_tokenizer()\n",
    "text = \"Sean Bean has a hard time leaving his role as Eddard Stark . He vows to get revenge against those that assisted in his execution , starting with George R. R. Martin\"\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10948"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Size : 42\n",
      "Embedded Tokens Shape: torch.Size([42, 1536])\n"
     ]
    }
   ],
   "source": [
    "# 3. Embedding 클래스 정의\n",
    "class Embedding:\n",
    "    def __init__(self, input_dim: int, output_dim: int) -> None:\n",
    "        \"\"\"\n",
    "        Custom Embedding 레이어 초기화\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.weights: Tensor = torch.randn(input_dim, output_dim) * 0.01\n",
    "        self.grad_weights: Tensor = torch.zeros_like(self.weights)\n",
    "\n",
    "    def forward(self, input_indices: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        순전파 과정\n",
    "        \"\"\"\n",
    "        self.input_indices = input_indices\n",
    "        self.output = self.weights[input_indices]\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        역전파 과정\n",
    "        \"\"\"\n",
    "        grad_flat = grad_output.view(-1, self.output_dim)\n",
    "        input_flat = self.input_indices.view(-1)\n",
    "        self.grad_weights.zero_()\n",
    "        self.grad_weights.index_add_(0, input_flat, grad_flat)\n",
    "        return None\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"CustomEmbedding\"\n",
    "\n",
    "# Embedding 테스트\n",
    "vocab_size:int = len(tokenizer.token_map)\n",
    "embed_size:int = 1536\n",
    "embedding_layer = Embedding(vocab_size, embed_size)\n",
    "embedded_tokens = embedding_layer.forward(torch.tensor(encoded))\n",
    "print(f\"Token Size : {len(encoded)}\")\n",
    "print(f\"Embedded Tokens Shape: {embedded_tokens.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Size : 42\n",
      "Embedded Tokens Shape: torch.Size([42, 1536])\n",
      "Output with Positional Encoding Shape: torch.Size([42, 1536])\n"
     ]
    }
   ],
   "source": [
    "# 4. Positional Encoding 클래스 정의\n",
    "class PositionalEncoding:\n",
    "    def __init__(self, max_seq_len: int, embed_size: int):\n",
    "        \"\"\"\n",
    "        위치 인코딩 초기화\n",
    "        \"\"\"\n",
    "        self.embed_size = embed_size\n",
    "        self.pos_encoding = torch.zeros(max_seq_len, embed_size)\n",
    "\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        self.pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pos_encoding = self.pos_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        순전파 과정 (2차원 반환)\n",
    "        \"\"\"\n",
    "        seq_length = x.size(1)  # 입력의 시퀀스 길이\n",
    "        x_with_pos = x + self.pos_encoding[:, :seq_length, :]  # 위치 인코딩 추가\n",
    "        return x_with_pos.view(-1, self.embed_size)  # 2차원 텐서로 변환\n",
    "\n",
    "# Positional Encoding 테스트\n",
    "\n",
    "vocab_size:int = len(tokenizer.token_map)\n",
    "embed_size:int = 1536\n",
    "embedding_layer = Embedding(vocab_size, embed_size)\n",
    "embedded_tokens = embedding_layer.forward(torch.tensor(encoded))\n",
    "\n",
    "max_seq_len = len(encoded)\n",
    "pos_encoding_layer = PositionalEncoding(max_seq_len, embed_size)\n",
    "output_with_pos_encoding = pos_encoding_layer.forward(embedded_tokens.unsqueeze(0))\n",
    "\n",
    "print(f\"Token Size : {len(encoded)}\")\n",
    "print(f\"Embedded Tokens Shape: {embedded_tokens.shape}\")\n",
    "print(f\"Output with Positional Encoding Shape: {output_with_pos_encoding.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
