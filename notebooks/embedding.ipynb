{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class BytePairTokenizer:\n",
    "    def __init__(self, data_path:str=None) -> None:\n",
    "        \"\"\"\n",
    "        BytePairTokenizer object\n",
    "        \"\"\"\n",
    "        if data_path:\n",
    "            self.load_model(data_path)\n",
    "            return\n",
    "        \n",
    "        self.special_tokens:Dict[str, int] = {\n",
    "            '<BOT>': 0,  # Beginning of Text\n",
    "            '<EOT>': 1,   # End of Text\n",
    "            '</w>': 2     # end of word\n",
    "        }\n",
    "        self.inv_special_tokens:Dict[int, str] = {i: t for t, i in self.special_tokens.items()}\n",
    "\n",
    "        self.token_map: Dict[str, int] = self.special_tokens.copy()\n",
    "        self.inv_map: Dict[int, str] = self.inv_special_tokens.copy()\n",
    "        self.bpe_codes: Dict[Tuple[str, str], int] = {}\n",
    "    \n",
    "    def train(self, corpus: List[str], num_merges: int, verbose:bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Train the Byte Pair Tokenizer to process sentences.\n",
    "        \"\"\"\n",
    "        # Build the vocabulary: map token sequences to their frequencies\n",
    "        vocab = {}\n",
    "        if verbose:\n",
    "            print(\"Building vocabulary...\")\n",
    "        for sentence in tqdm(corpus):\n",
    "            # Split sentence into words with leading whitespace preserved\n",
    "            words = re.findall(r'\\s*\\S+|\\s+', sentence)\n",
    "            for word in words:\n",
    "                # Skip special tokens\n",
    "                if word in self.special_tokens.keys():\n",
    "                    continue\n",
    "                chars = list(word) + ['</w>']\n",
    "                word_tuple = tuple(chars)\n",
    "                vocab[word_tuple] = vocab.get(word_tuple, 0) + 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Vocabulary built.\\nTraining BPE...\")\n",
    "        token_id = len(self.token_map)  # Starting token ID\n",
    "        symbols = set()\n",
    "        for word_tuple in vocab.keys():\n",
    "            symbols.update(word_tuple)\n",
    "        for symbol in symbols:\n",
    "            if symbol not in self.token_map:\n",
    "                self.token_map[symbol] = token_id\n",
    "                token_id += 1\n",
    "        self.inv_map = {i: t for t, i in self.token_map.items()}\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Token map built.\\nMerging tokens...\")\n",
    "        # Perform BPE merges\n",
    "        for i in tqdm(range(num_merges)):\n",
    "            pairs = self._get_pair_counts(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            vocab = self._merge_vocab(best_pair, vocab)\n",
    "            self.bpe_codes[best_pair] = i # Record the BPE merge rule\n",
    "            new_symbol = ''.join(best_pair)\n",
    "            if new_symbol not in self.token_map:\n",
    "                self.token_map[new_symbol] = token_id\n",
    "                token_id += 1\n",
    "                self.inv_map[self.token_map[new_symbol]] = new_symbol\n",
    "    \n",
    "    def _get_pair_counts(self, vocab: Dict[Tuple[str], int]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"\n",
    "        Get counts of symbol pairs in the vocabulary\n",
    "        \"\"\"\n",
    "        pairs = {}\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + freq\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_vocab_single(self, pair: Tuple[str, str], vocab: Dict[Tuple[str], int]) -> Dict[Tuple[str], int]:\n",
    "        \"\"\"\n",
    "        Merge all occurrences of the given pair in the vocabulary\n",
    "        \"\"\"\n",
    "        new_vocab = {}\n",
    "        bigram = ''.join(pair)\n",
    "        for word, freq in vocab.items():\n",
    "            w = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                # Merge the pair if found\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                    w.append(bigram)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    w.append(word[i])\n",
    "                    i += 1\n",
    "            new_vocab[tuple(w)] = freq\n",
    "        return new_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_word(args):\n",
    "        pair, word_freq = args\n",
    "        word, freq = word_freq\n",
    "        bigram = ''.join(pair)\n",
    "        w = []\n",
    "        i = 0\n",
    "        while i < len(word):\n",
    "            if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                w.append(bigram)\n",
    "                i += 2\n",
    "            else:\n",
    "                w.append(word[i])\n",
    "                i += 1\n",
    "        return tuple(w), freq\n",
    "    \n",
    "    def _merge_vocab(self, pair: Tuple[str, str], vocab: Dict[Tuple[str], int]) -> Dict[Tuple[str], int]:\n",
    "        \"\"\"\n",
    "        Parallel merge of all occurrences of the given pair in the vocabulary using multiprocessing.\n",
    "        \"\"\"\n",
    "        with Pool() as pool:\n",
    "            results = pool.map(self._process_word, [(pair, word_freq) for word_freq in vocab.items()])\n",
    "\n",
    "        new_vocab = {word: freq for word, freq in results}\n",
    "        return new_vocab\n",
    "    \n",
    "    def _get_pairs(self, word: List[str]) -> set:\n",
    "        \"\"\"\n",
    "        Return a set of symbol pairs in a word\n",
    "        \"\"\"\n",
    "        pairs = set()\n",
    "        for i in range(len(word) - 1):\n",
    "            pairs.add((word[i], word[i + 1]))\n",
    "        return pairs\n",
    "    \n",
    "    def _apply_bpe(self, word: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Apply BPE to a list of symbols (a word)\n",
    "        \"\"\"\n",
    "        word = word.copy()\n",
    "        pairs = self._get_pairs(word)\n",
    "        while True:\n",
    "            if not pairs:\n",
    "                break\n",
    "            # Find the highest priority pair to merge\n",
    "            min_pair = None\n",
    "            min_rank = float('inf')\n",
    "            for pair in pairs:\n",
    "                if pair in self.bpe_codes:\n",
    "                    rank = self.bpe_codes[pair]\n",
    "                    if rank < min_rank:\n",
    "                        min_rank = rank\n",
    "                        min_pair = pair\n",
    "            if min_pair is None:\n",
    "                break\n",
    "            # Merge the best pair\n",
    "            new_symbol = ''.join(min_pair)\n",
    "            i = 0\n",
    "            while i < len(word) - 1:\n",
    "                if word[i] == min_pair[0] and word[i + 1] == min_pair[1]:\n",
    "                    word[i:i + 2] = [new_symbol]\n",
    "                    i = max(i - 1, 0)  # Restart from the previous position after a merge\n",
    "                else:\n",
    "                    i += 1\n",
    "            pairs = self._get_pairs(word)\n",
    "        return word\n",
    "    \n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into BPE tokens with leading whitespace preserved\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        words = re.findall(r'\\s*\\S+|\\s+', text)\n",
    "        for word in words:\n",
    "            chars = list(word) + ['</w>']\n",
    "            bpe_word = self._apply_bpe(chars)\n",
    "            tokens.extend(bpe_word)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, data: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode text data into a list of token IDs\n",
    "        \"\"\"\n",
    "        str_list = self.split_text(data)\n",
    "        token_list = [self.token_map[tok] for tok in str_list]\n",
    "        return token_list\n",
    "    \n",
    "    def decode(self, data: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into text\n",
    "        \"\"\"\n",
    "        tokens = [self.inv_map[i] for i in data]\n",
    "        text = ''\n",
    "        for token in tokens:\n",
    "            if token != '</w>':\n",
    "                text += token.replace('</w>', '')\n",
    "        return text\n",
    "\n",
    "    def save_model(self, target_path:str) -> None:\n",
    "        \"\"\"\n",
    "        Save the model to a file as json file\n",
    "        the json will look like\n",
    "        {\n",
    "            token_map : {...},\n",
    "            bpe_codes : {...}\n",
    "        }\n",
    "        The special tokens are not necessary for simple encoding/decoding\n",
    "        hence it is omitted from the model\n",
    "        \"\"\"\n",
    "        with open(target_path, 'w', encoding=\"UTF-8\") as f:\n",
    "            json.dump({\n",
    "                'token_map': self.token_map,\n",
    "                'bpe_codes': {json.dumps(list(k)): v for k, v in self.bpe_codes.items()}\n",
    "            }, f,\n",
    "             indent=4,\n",
    "              ensure_ascii=False)\n",
    "    \n",
    "    def load_model(self, model_path:str, encoding=\"UTF-8\") -> None:\n",
    "        \"\"\"\n",
    "        Load the model from a json file\n",
    "        JSON doesn't allow tuple object as key\n",
    "        hence the tuple keys are converted to string before saving\n",
    "        and converted back to tuple when loading\n",
    "        \"\"\"\n",
    "        with open(model_path, 'r') as f:\n",
    "            model = json.load(f)\n",
    "        self.token_map = model['token_map']\n",
    "        self.inv_map = {i: t for t, i in self.token_map.items()}\n",
    "        self.bpe_codes = {tuple(json.loads(k)): v for k, v in model['bpe_codes'].items()}\n",
    "\n",
    "def load_tokenizer(path:str = None) -> BytePairTokenizer:\n",
    "    \"\"\"\n",
    "    Load the BytePairTokenizer model from the model folder\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        model_path:str = os.path.join(os.getcwd(), 'model', 'tokenizer.json')\n",
    "    else:\n",
    "        model_path:str = path\n",
    "    tokenizer = BytePairTokenizer(model_path)\n",
    "    # tokenizer.load_model(model_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
